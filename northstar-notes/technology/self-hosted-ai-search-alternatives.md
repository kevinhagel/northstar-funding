# Self-Hosted AI Search Alternatives to Tavily/Perplexity

**Date**: 2025-11-07
**Context**: Replace paid Tavily API with local AI search on Mac Studio
**Hardware**: Mac Studio M4 Max (32 GPU cores, 36 GB RAM)
**Existing Infrastructure**: SearXNG (port 8080), LM Studio/Ollama, Docker

---

## Executive Summary

**Yes, you can replace Tavily with a completely local AI search system!**

### The Stack You Already Have
```
‚úÖ SearXNG (port 8080) - Multi-engine search aggregator
‚úÖ LM Studio (port 1234) - Local LLM inference
‚úÖ Docker - Container infrastructure
‚úÖ 32 GPU cores - Massive compute for AI reasoning
```

### What You Need to Add
```
‚Üí Perplexica or Open WebUI - AI search frontend/orchestrator
‚Üí Optionally: Ollama (alongside LM Studio) - For open-source LLM support
```

**Result**: Zero API costs, complete privacy, full control, leverages your GPU power

---

## Top 3 Recommendations for Your Use Case

### ü•á #1: Perplexica (Best for Your Needs)

**What it is**: Open-source AI-powered answer engine (Perplexity.ai clone)

**Why it's perfect for you:**
- ‚úÖ **Already integrates with SearXNG** (your existing search)
- ‚úÖ **Works with Ollama OR OpenAI-compatible APIs** (LM Studio!)
- ‚úÖ **Docker deployment** (fits your infrastructure)
- ‚úÖ **Privacy-focused** (all local)
- ‚úÖ **Active development** (13k+ GitHub stars)
- ‚úÖ **6 specialized focus modes** (general, academic, Reddit, YouTube, Wolfram, writing)

**Architecture:**
```
User Query
    ‚Üì
Perplexica (Docker container, port 3000)
    ‚Üì
LM Studio API (port 1234) - AI reasoning
    ‚Üì
SearXNG (port 8080) - Web search
    ‚Üì
LM Studio API - Synthesize answer with citations
```

**Installation (Docker - 1 command):**
```bash
docker run -d \
  -p 3000:3000 \
  -v perplexica-data:/home/perplexica/data \
  -v perplexica-uploads:/home/perplexica/uploads \
  --name perplexica \
  itzcrazykns1337/perplexica:latest
```

**Configuration:**
Edit `config.toml` to point to:
- SearXNG: `http://192.168.1.10:8080`
- LM Studio: `http://192.168.1.10:1234/v1` (OpenAI-compatible)

**Memory Usage:**
- Perplexica container: ~200-300 MB
- Total impact: Minimal (you have 30 GB free)

**GitHub**: https://github.com/ItzCrazyKns/Perplexica

---

### ü•à #2: Open WebUI (Most Feature-Rich)

**What it is**: Complete AI platform with built-in web search RAG

**Why consider it:**
- ‚úÖ **Full-featured AI interface** (like ChatGPT UI)
- ‚úÖ **Built-in RAG with web search** (supports SearXNG)
- ‚úÖ **Document library** (upload PDFs, manage knowledge base)
- ‚úÖ **Multiple LLM support** (Ollama, OpenAI API compatible)
- ‚úÖ **User management** (multi-user, authentication)
- ‚úÖ **Works completely offline**

**Architecture:**
```
User ‚Üí Open WebUI (port 8080)
         ‚Üì
    [RAG Pipeline]
         ‚Üì
    ‚îú‚îÄ‚îÄ SearXNG (web search)
    ‚îú‚îÄ‚îÄ LM Studio (reasoning)
    ‚îî‚îÄ‚îÄ Document Library (local files)
```

**Installation (Docker Compose):**
```yaml
version: '3'
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "8080:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - SEARXNG_QUERY_URL=http://searxng:8080/search?q=<query>
      - OLLAMA_BASE_URL=http://host.docker.internal:1234/v1
```

**Trade-off:**
- More features than Perplexica (full AI platform)
- Slightly heavier (500-800 MB RAM)
- Might be overkill if you only need search

**GitHub**: https://github.com/open-webui/open-webui

---

### ü•â #3: Farfalle (Lightweight Alternative)

**What it is**: Minimal AI search engine (Perplexity clone)

**Why consider it:**
- ‚úÖ **Lightweight** (~100-200 MB RAM)
- ‚úÖ **Supports SearXNG**
- ‚úÖ **Local LLMs via Ollama**
- ‚úÖ **Docker deployment**
- ‚úÖ **Agent-based search planning**

**Trade-off:**
- Less mature than Perplexica
- Fewer focus modes
- Smaller community

**GitHub**: https://github.com/rashadphz/farfalle

---

## Detailed Comparison Matrix

| Feature | Perplexica | Open WebUI | Farfalle | Tavily API |
|---------|-----------|------------|----------|------------|
| **Cost** | Free | Free | Free | $1-2/1000 searches |
| **Privacy** | 100% local | 100% local | 100% local | Cloud-based |
| **SearXNG Integration** | ‚úÖ Native | ‚úÖ Built-in | ‚úÖ Yes | ‚ùå No |
| **LM Studio Compatible** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Via Ollama | N/A |
| **Docker Deployment** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | N/A |
| **Memory Usage** | ~300 MB | ~800 MB | ~200 MB | 0 (API) |
| **Setup Complexity** | Easy | Medium | Easy | Easiest (API key) |
| **Maintenance** | Low | Medium | Low | None (managed) |
| **Feature Set** | Search focused | Full AI platform | Search only | Search API |
| **Citations** | ‚úÖ Yes | ‚úÖ Yes | ‚ö†Ô∏è Basic | ‚úÖ Yes |
| **Focus Modes** | ‚úÖ 6 modes | ‚ö†Ô∏è Limited | ‚ùå No | ‚ùå No |
| **GitHub Stars** | 13k+ | 45k+ | 500+ | N/A |
| **Active Development** | ‚úÖ Very active | ‚úÖ Very active | ‚úÖ Active | N/A |

---

## How It Works: Local AI Search Architecture

### Traditional (Current): Tavily API
```
Your App
    ‚Üì
Generate Query (LM Studio)
    ‚Üì
Call Tavily API ($$$) ‚Üí Web Search ‚Üí AI Processing ‚Üí Answer
    ‚Üì
Receive Answer with Citations
```

**Cost**: $1-2 per 1000 searches
**Privacy**: Data sent to Tavily servers
**Control**: Limited (rate limits, API changes)

### Self-Hosted: Perplexica + SearXNG + LM Studio
```
Your App
    ‚Üì
Generate Query (LM Studio @ port 1234)
    ‚Üì
Send to Perplexica (@ port 3000)
    ‚Üì
Perplexica orchestrates:
    1. Search planning (LM Studio)
    2. Web search (SearXNG @ port 8080)
    3. Result extraction (scrape content)
    4. Answer synthesis (LM Studio)
    5. Citation formatting
    ‚Üì
Receive Answer with Citations (all local!)
```

**Cost**: $0 (hardware you already own)
**Privacy**: 100% local (nothing leaves Mac Studio)
**Control**: Complete (customize everything)

---

## Performance Estimates on Your Mac Studio

### Search + Answer Generation Time

**Using Qwen2.5-0.5B (current model):**
```
Query generation:        200-300ms
Search planning:         100-200ms
SearXNG search:         500-1000ms (network bound)
Result extraction:      1000-2000ms (fetch ~5-10 pages)
Answer synthesis:       500-1000ms
Total:                  ~3-5 seconds per search
```

**Using Qwen2.5-3B (better quality):**
```
Query generation:        500-700ms
Search planning:         200-400ms
SearXNG search:         500-1000ms
Result extraction:      1000-2000ms
Answer synthesis:       1000-1500ms
Total:                  ~4-6 seconds per search
```

**Compare to Tavily API:**
- Tavily: ~2-3 seconds (but $$$)
- Local: ~3-6 seconds (free, private)

**Verdict**: Slightly slower but totally acceptable for funding discovery (not real-time chat)

---

## Memory Budget Analysis

### Current Usage (Before AI Search)
```
System:              ~3 GB
Docker:              ~2 GB
  - PostgreSQL:      ~500 MB
  - Qdrant:          ~500 MB
  - pgAdmin:         ~300 MB
  - SearXNG:         ~200 MB
LM Studio:           ~1 GB
Available:           ~30 GB
```

### After Adding Perplexica
```
System:              ~3 GB
Docker:              ~2.5 GB
  - PostgreSQL:      ~500 MB
  - Qdrant:          ~500 MB
  - pgAdmin:         ~300 MB
  - SearXNG:         ~200 MB
  - Perplexica:      ~300 MB ‚Üê NEW
LM Studio:           ~1 GB
Available:           ~29.5 GB (still 82% free)
```

### After Adding Open WebUI (alternative)
```
System:              ~3 GB
Docker:              ~3 GB
  - PostgreSQL:      ~500 MB
  - Qdrant:          ~500 MB
  - pgAdmin:         ~300 MB
  - SearXNG:         ~200 MB
  - Open WebUI:      ~800 MB ‚Üê NEW
LM Studio:           ~1 GB
Available:           ~29 GB (still 81% free)
```

**Conclusion**: Even Open WebUI (heaviest option) barely impacts your massive headroom.

---

## GPU Utilization Analysis

### Current GPU Usage
```
Docker services:     <1% GPU (databases don't use GPU)
LM Studio (idle):    0% GPU
LM Studio (query):   15-20% GPU (250ms burst)
Available:           80-100% GPU most of the time
```

### With AI Search (Perplexica + LM Studio)

**During AI Search Request:**
```
Phase 1: Query planning
  - LM Studio: 15-20% GPU for 100-200ms

Phase 2: Web search
  - SearXNG: 0% GPU (network I/O)

Phase 3: Answer synthesis
  - LM Studio: 15-20% GPU for 500-1000ms

Total GPU time: ~700-1200ms at 15-20% utilization
Peak GPU: 20%
Average GPU: 5-10% (amortized over 3-5 second request)
```

**Concurrent Searches:**
Your 32 GPU cores can handle **10-20 concurrent AI search requests** before GPU becomes bottleneck.

**Conclusion**: GPU is NOT a constraint for AI search workload.

---

## Integration with Your Java Application

### Option 1: Use Perplexica API Directly

**Perplexica exposes REST API:**
```java
// Instead of Tavily
String tavilyUrl = "https://api.tavily.com/search";

// Use Perplexica
String perplexicaUrl = "http://192.168.1.10:3000/api/search";

// Request format (similar to Tavily)
{
  "query": "EU Horizon grants for educational infrastructure Bulgaria",
  "focus_mode": "webSearch",  // or "academic", "reddit", etc.
  "chat_model": "default"     // uses configured LLM
}

// Response format
{
  "message": "Based on my search...",
  "sources": [
    {
      "title": "Horizon Europe",
      "url": "https://...",
      "snippet": "..."
    }
  ]
}
```

**LangChain4j Integration:**
```java
// Custom Perplexica search tool
@Tool("Search the web using local AI search")
public String searchWeb(@ToolParam("query") String query) {
    String perplexicaUrl = "http://192.168.1.10:3000/api/search";

    // Call Perplexica API
    WebClient client = WebClient.create();
    PerplexicaResponse response = client.post()
        .uri(perplexicaUrl)
        .bodyValue(Map.of(
            "query", query,
            "focus_mode", "webSearch"
        ))
        .retrieve()
        .bodyToMono(PerplexicaResponse.class)
        .block();

    return response.getMessage(); // AI-synthesized answer
}
```

### Option 2: Keep Tavily for Critical Path, Use Perplexica for Experiments

**Hybrid approach:**
```java
@Configuration
public class SearchConfig {
    @Bean
    @Primary
    public SearchService primarySearch() {
        return new TavilySearchService(); // Paid, reliable
    }

    @Bean
    @Qualifier("experimental")
    public SearchService experimentalSearch() {
        return new PerplexicaSearchService(); // Free, local
    }
}
```

**Use Perplexica for:**
- Development/testing
- Background research
- Non-critical queries
- High-volume experiments

**Use Tavily for:**
- Production critical path
- Time-sensitive queries
- Maximum reliability

---

## Recommended Implementation Plan

### Phase 1: Parallel Testing (This Week)

**Goal**: Test Perplexica alongside Tavily, compare quality

**Steps:**
1. **Fix SearXNG configuration** (currently failing after power outage)
   ```bash
   ssh macstudio
   docker logs searxng  # Check error
   docker restart searxng  # Try restart
   # If fails, check /docker/searxng/settings.yml
   ```

2. **Deploy Perplexica** (single Docker command)
   ```bash
   docker run -d \
     -p 3000:3000 \
     -v perplexica-data:/home/perplexica/data \
     --name perplexica \
     itzcrazykns1337/perplexica:latest
   ```

3. **Configure Perplexica**
   - Edit config.toml
   - Point to SearXNG: http://localhost:8080
   - Point to LM Studio: http://localhost:1234/v1

4. **Test from browser**
   - Navigate to http://192.168.1.10:3000
   - Try sample queries
   - Compare to Tavily results

5. **Measure performance**
   - Query latency
   - Answer quality
   - Citation accuracy

**Expected Result**:
- Perplexica 80-90% as good as Tavily
- 2-3x slower (3-5s vs 2-3s)
- $0 cost

### Phase 2: Java Integration (Next Week)

**Goal**: Create PerplexicaSearchService alongside TavilySearchService

**Steps:**
1. Create `PerplexicaSearchService` implementing `SearchService`
2. Add Perplexica API client with WebClient
3. Write integration tests comparing both services
4. Document API differences

**Expected Result**:
- Both services work in parallel
- Can switch between them with configuration

### Phase 3: Decision Point (Week 3)

**Decide based on:**

**If Perplexica is 80%+ quality:**
‚Üí Switch to Perplexica for all queries (save $$$)

**If Perplexica is 60-80% quality:**
‚Üí Use hybrid approach:
  - Perplexica for bulk/background queries
  - Tavily for critical path

**If Perplexica is <60% quality:**
‚Üí Keep Tavily, use Perplexica only for experiments

### Phase 4: Optimization (Ongoing)

**If using Perplexica:**
1. Upgrade to larger model for better reasoning (3B or 7B)
2. Fine-tune search prompts for funding discovery domain
3. Add custom focus mode for "funding_research"
4. Optimize SearXNG configuration (enable specific engines)

---

## Model Selection for AI Search

### Current: Qwen2.5-0.5B
**Performance for AI Search:**
- ‚úÖ Query planning: Fast (100-200ms)
- ‚ö†Ô∏è Answer synthesis: Basic quality
- ‚úÖ Citation extraction: Adequate
- ‚ö†Ô∏è Multi-document reasoning: Limited

**Verdict**: Adequate for testing, upgrade for production

### Recommended: Qwen2.5-3B
**Performance for AI Search:**
- ‚úÖ Query planning: Good (200-400ms)
- ‚úÖ Answer synthesis: Much better quality
- ‚úÖ Citation extraction: Excellent
- ‚úÖ Multi-document reasoning: Strong

**Memory**: 2.5 GB (you have 30 GB free)
**Speed**: ~100-150 tokens/sec
**Verdict**: Best balance for AI search

### Alternative: Qwen2.5-7B (Maximum Quality)
**Performance for AI Search:**
- ‚úÖ Query planning: Excellent (400-600ms)
- ‚úÖ Answer synthesis: Best quality
- ‚úÖ Citation extraction: Perfect
- ‚úÖ Multi-document reasoning: Excellent

**Memory**: 5 GB
**Speed**: ~40-60 tokens/sec
**Verdict**: Use if quality > speed

**Recommendation**: Start with 3B, upgrade to 7B if needed

---

## Cost Comparison

### Tavily API Costs (Estimated)
```
Searches per month:    10,000 (example: 50/day * 200 days)
Cost per 1000:         $2.00
Monthly cost:          $20/month
Annual cost:           $240/year
```

### Self-Hosted Costs
```
Hardware:              $0 (already own Mac Studio)
Electricity:           ~$5-10/month (Mac Studio 24/7)
Maintenance:           ~1 hour/month (updates, monitoring)
Annual cost:           ~$60-120/year (electricity only)

Savings:               $120-180/year
```

### Break-even Analysis
**If Tavily costs $20/month:**
- Year 1 savings: $120-180
- Year 2 savings: $240 (full Tavily cost)
- Year 3+ savings: $240/year

**ROI**: Pays for itself in electricity savings alone

**Plus intangibles:**
- Complete privacy (no data leaves Mac Studio)
- No rate limits
- Full control and customization
- Learning opportunity

---

## Troubleshooting Guide

### Issue: SearXNG Not Working
**Symptoms**: "Invalid settings.yml" error

**Solution:**
```bash
ssh macstudio
cd ~/docker  # Or wherever docker-compose.yml is
docker logs searxng --tail 100

# Check settings.yml
docker exec -it searxng cat /etc/searxng/settings.yml

# If corrupted, restore from backup or recreate
docker stop searxng
docker rm searxng
docker compose up -d searxng
```

### Issue: Perplexica Can't Connect to LM Studio
**Symptoms**: "Connection refused" or timeout

**Solution:**
```bash
# Verify LM Studio is running
curl http://192.168.1.10:1234/v1/models

# Check Perplexica config
docker exec -it perplexica cat /home/perplexica/config.toml

# Fix config (should use host.docker.internal or 192.168.1.10)
# Restart Perplexica
docker restart perplexica
```

### Issue: Slow Performance
**Symptoms**: Searches taking >10 seconds

**Solutions:**
1. Upgrade to faster model (3B instead of 0.5B)
2. Reduce number of search results fetched
3. Enable GPU acceleration (should be automatic)
4. Check network speed (SearXNG is network-bound)

---

## Conclusion

### Yes, You Can Replace Tavily!

**Best Option for Your Use Case: Perplexica**

**Why:**
1. ‚úÖ Integrates with your existing SearXNG
2. ‚úÖ Works with LM Studio (OpenAI-compatible API)
3. ‚úÖ Docker deployment (fits infrastructure)
4. ‚úÖ Minimal resource usage (~300 MB)
5. ‚úÖ Active development and community
6. ‚úÖ Specialized focus modes
7. ‚úÖ Complete privacy (100% local)
8. ‚úÖ Zero ongoing costs

**Trade-offs:**
- ‚ö†Ô∏è Slightly slower (3-5s vs 2-3s for Tavily)
- ‚ö†Ô∏è Requires maintenance (updates, monitoring)
- ‚ö†Ô∏è Quality depends on LLM size (upgrade to 3B recommended)

### Next Steps

1. **Fix SearXNG** (currently failing)
2. **Deploy Perplexica** (1 Docker command)
3. **Test in browser** (http://192.168.1.10:3000)
4. **Compare to Tavily** (quality, speed, citations)
5. **Integrate with Java** (PerplexicaSearchService)
6. **Decide**: Full migration vs hybrid approach

**Your Mac Studio with 32 GPU cores makes this viable.** You have the compute power to run high-quality AI search locally.

---

**Document Version**: 1.0
**Last Updated**: 2025-11-07
**Recommended Action**: Deploy Perplexica this week for testing
**Expected Savings**: $120-240/year vs Tavily
