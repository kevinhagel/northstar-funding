spring:
  cache:
    caffeine:
      spec: maximumSize=1000,expireAfterWrite=24h

# Query Generation Configuration
query-generation:
  # Query limits
  max-queries-limit: 50
  min-queries-limit: 1
  default-queries: 10

  # LLM Configuration
  # PRIMARY: LM Studio (port 1234) - proven reliable with Perplexica
  # FALLBACK: Ollama (port 11434) - has parallelism issues with Perplexica
  llm:
    base-url: http://192.168.1.10:1234/v1
    api-key: not-needed
    timeout-seconds: 60
    model-name: llama-3.1-8b-instruct  # LM Studio model ID
    max-tokens: 500
    temperature: 0.7
    log-requests: false
    log-responses: false

  # Cache Configuration
  cache:
    enabled: true
    max-size: 1000
    ttl-hours: 24
    record-stats: true

  # Persistence Configuration
  persistence:
    async: true
    enabled: true
